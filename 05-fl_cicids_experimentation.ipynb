{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee2ad1b-d745-4a7b-bc06-8fe8d614d8ff",
   "metadata": {},
   "source": [
    "# Federated Learning Case Study\n",
    "\n",
    "## 05. FL CICIDS - Experimentation with different client configurations\n",
    "\n",
    "In this notebook, we devise a set of client configurations based on how the training data could be split across these. We use four schemes: individual IPs, individual attacks, group of IPs (based on similar OS), and group of attacks (based on name/type similarity).\n",
    "\n",
    "We use our previous model and can deploy each of these scenarios to the FL process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2c4c1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDIVIDUAL_IP\n"
     ]
    }
   ],
   "source": [
    "### THIS SECTION NEEDS TO BE SET TO DETERMINE WHICH CONFIGURATION METHOD TO UTILISE\n",
    "\n",
    "AVAILABLE_METHODS = ['INDIVIDUAL_IP', 'GROUP_IP', 'INDIVIDUAL_ATTACK', 'GROUP_ATTACK', 'STRATIFIED']\n",
    "METHOD = AVAILABLE_METHODS[0]\n",
    "NUM_OF_STRATIFIED_CLIENTS = 10 # only applies to stratified method\n",
    "print (METHOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fefb66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf064e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.read_pickle('full_cicids_df.pkl')\n",
    "# feature_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b41e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Pre-processing to tidy up NaN values\n",
    "feature_df2 = feature_df.replace([np.inf, - np.inf], np.nan)\n",
    "feature_df2 = feature_df2.dropna()    \n",
    "label = feature_df2['Label']\n",
    "le = preprocessing.LabelEncoder()\n",
    "label_n = le.fit_transform(label.values)\n",
    "\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(feature_df2, label_n, stratify=label_n, test_size=0.2, random_state=42)\n",
    "feature_df2 = feature_df2.drop(['Flow ID', 'Src IP', 'Dst IP', 'Timestamp', 'Label'], axis=1)\n",
    "\n",
    "# Scaling of features to fit within defined range\n",
    "scaled_features = StandardScaler().fit_transform(feature_df2.values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features, label_n, stratify=label_n, test_size=0.2, random_state=42)\n",
    "\n",
    "print (\"Train:\", X_train.shape, y_train.shape)\n",
    "print (\"Test:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbccac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_ip_addresses = [ '192.168.10.25', '192.168.10.15', '192.168.10.14', '192.168.10.8', '192.168.10.5', '192.168.10.9', '192.168.10.12', '192.168.10.16', '192.168.10.17', '192.168.10.19', '192.168.10.51', '192.168.10.50']\n",
    "print (\"known ip addresses:\", known_ip_addresses)\n",
    "os_groups = [ ['192.168.10.25'],  #macOS\n",
    "                ['192.168.10.15', '192.168.10.14', '192.168.10.8', '192.168.10.5', '192.168.10.9'], #win\n",
    "                ['192.168.10.12', '192.168.10.16', '192.168.10.17', '192.168.10.19', '192.168.10.51', '205.174.165.66'], #ubuntu\n",
    "                ['192.168.10.50', '205.174.165.68'], # webserver\n",
    "                ]\n",
    "print (\"os_groups:\", os_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c12a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks = label.unique()\n",
    "attack_groups = [['BENIGN'],\n",
    "                    ['DoS slowloris',\n",
    "       'DoS slowloris - Attempted', 'DoS Slowhttptest',\n",
    "       'DoS Slowhttptest - Attempted', 'DoS Hulk', 'DoS Hulk - Attempted',\n",
    "       'DoS GoldenEye', 'DDoS'],\n",
    "                    ['FTP-Patator', 'SSH-Patator', 'FTP-Patator - Attempted',\n",
    "       'SSH-Patator - Attempted'],\n",
    "                    ['Heartbleed'],\n",
    "                    ['Web Attack - Brute Force', 'Web Attack - Brute Force - Attempted', 'Web Attack - XSS - Attempted', 'Web Attack - XSS',\n",
    "       'Web Attack - Sql Injection'], \n",
    "                    ['Bot - Attempted', 'Bot'],\n",
    "                    ['PortScan'],\n",
    "                    ['Infiltration - Attempted', 'Infiltration']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe7d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_X_train = []\n",
    "fl_y_train = []\n",
    "\n",
    "if METHOD == AVAILABLE_METHODS[0]:\n",
    "    #### Group by individual IP (src and dst)\n",
    "    for i in known_ip_addresses:\n",
    "        new_i = [i]\n",
    "        print (new_i)\n",
    "        new_df_src = X_train_df[ X_train_df['Src IP'].isin(new_i) ]\n",
    "        new_df_dst = X_train_df[ X_train_df['Dst IP'].isin(new_i) ]\n",
    "        #new_df = pd.concat([new_df1, new_df2], axis=0)\n",
    "\n",
    "        X_np = np.vstack([ X_train[ new_df_src.index, : ], X_train[ new_df_dst.index, :] ])\n",
    "        y_np = np.hstack([ y_train[ new_df_src.index ], y_train[ new_df_dst.index ] ])\n",
    "\n",
    "        print (X_np.shape)\n",
    "        print (y_np.shape)\n",
    "\n",
    "        fl_X_train.append(X_np)\n",
    "        fl_y_train.append(y_np) \n",
    "elif METHOD == AVAILABLE_METHODS[1]:\n",
    "    #### Group by OS (src and dst)\n",
    "\n",
    "    for i in os_groups:\n",
    "        new_i = i\n",
    "        print (new_i)\n",
    "        new_df_src = X_train_df[ X_train_df['Src IP'].isin(new_i) ]\n",
    "        new_df_dst = X_train_df[ X_train_df['Dst IP'].isin(new_i) ]\n",
    "        #new_df = pd.concat([new_df1, new_df2], axis=0)\n",
    "\n",
    "        X_np = np.vstack([ X_train[ new_df_src.index, : ], X_train[ new_df_dst.index, :] ])\n",
    "        y_np = np.hstack([ y_train[ new_df_src.index ], y_train[ new_df_dst.index ] ])\n",
    "\n",
    "        print (X_np.shape)\n",
    "        print (y_np.shape)\n",
    "\n",
    "        fl_X_train.append(X_np)\n",
    "        fl_y_train.append(y_np) \n",
    "elif METHOD == AVAILABLE_METHODS[2]:\n",
    "    #### Group by individual attack\n",
    "    for i in attacks:\n",
    "        new_i = [i]\n",
    "        print (new_i)\n",
    "        new_df_attack = X_train_df[ X_train_df['Label'].isin(new_i) ]\n",
    "        #new_df = pd.concat([new_df1, new_df2], axis=0)\n",
    "\n",
    "        X_np = X_train[ new_df_attack.index, : ]\n",
    "        y_np = y_train[ new_df_attack.index ]\n",
    "\n",
    "        print (X_np.shape)\n",
    "        print (y_np.shape)\n",
    "\n",
    "        fl_X_train.append(X_np)\n",
    "        fl_y_train.append(y_np)\n",
    "elif METHOD == AVAILABLE_METHODS[3]:\n",
    "    #### Group by attack group\n",
    "\n",
    "    for i in attack_groups:\n",
    "        new_i = i\n",
    "        print (new_i)\n",
    "        new_df_attack_group = X_train_df[ X_train_df['Label'].isin(new_i) ]\n",
    "\n",
    "        X_np = X_train[ new_df_attack_group.index, : ]\n",
    "        y_np = y_train[ new_df_attack_group.index ]\n",
    "\n",
    "        print (X_np.shape)\n",
    "        print (y_np.shape)\n",
    "\n",
    "        fl_X_train.append(X_np)\n",
    "        fl_y_train.append(y_np) \n",
    "elif METHOD == AVAILABLE_METHODS[4]:\n",
    "    ## 1. STRATIFIED SAMPLING\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    \n",
    "\n",
    "    skf = StratifiedKFold(n_splits=NUM_OF_STRATIFIED_CLIENTS, shuffle=True, random_state=42)\n",
    "    skf.get_n_splits(X_train, y_train)\n",
    "\n",
    "    for _, test_index in skf.split(X_train, y_train):\n",
    "        fl_X_train.append(X_train[test_index])\n",
    "        fl_y_train.append(y_train[test_index]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdad252",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f53dfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK IF THIS REMAINS THE SAME OR CHANGED\n",
    "NUM_OF_CLIENTS = len(fl_X_train)\n",
    "print (\"NUM_OF_CLIENTS:\", NUM_OF_CLIENTS)\n",
    "\n",
    "NUM_OF_ROUNDS = 5\n",
    "\n",
    "print (\"Checking data split groups\")\n",
    "for i in range(len(fl_X_train)):\n",
    "    print (i, ':', \"X shape\", fl_X_train[i].shape, \" Y shape:\" , fl_y_train[i].shape)\n",
    "\n",
    "print (\"Importing Federated Learning environment...\")\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(\"flwr\", fl.__version__)\n",
    "print(\"numpy\", np.__version__)\n",
    "print(\"tf\", tf.__version__)\n",
    "# Make TensorFlow log less verbose\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "class NumpyFlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, model, train_data, train_labels):\n",
    "        self.model = model\n",
    "        self.cid = cid\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        print (\"Client \", self.cid, \"Training...\")\n",
    "        self.model.fit(self.train_data, self.train_labels, epochs=1, batch_size=32)\n",
    "        print (\"Client \", self.cid, \"Training complete...\")\n",
    "        return self.model.get_weights(), len(self.train_data), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        print (\"Client \", self.cid, \"Evaluating...\")\n",
    "        loss, accuracy = self.model.evaluate(self.train_data, self.train_labels, batch_size=32)\n",
    "        print (\"Client \", self.cid, \"Evaluating complete...\", accuracy, loss)\n",
    "        return loss, len(self.train_data), {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "def client_fn(cid: str) -> NumpyFlowerClient:\n",
    "    \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
    "\n",
    "    # Load model\n",
    "    #model = tf.keras.applications.MobileNetV2((32, 32, 3), classes=10, weights=None)\n",
    "    #model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    print (\"Client ID:\", cid)\n",
    "\n",
    "    model = Sequential([\n",
    "      #Flatten(input_shape=(79,1)),\n",
    "      Flatten(input_shape=(fl_X_train[0].shape[1] , 1)),\n",
    "      Dense(256, activation='sigmoid'),\n",
    "      Dense(128, activation='sigmoid'), \n",
    "      #Dense(18, activation='sigmoid'),  \n",
    "      Dense(len(label.unique()), activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    partition_id = int(cid)\n",
    "    X_train_c = fl_X_train[partition_id]\n",
    "    y_train_c = fl_y_train[partition_id]\n",
    "\n",
    "    # Create a  single Flower client representing a single organization\n",
    "    return NumpyFlowerClient(cid, model, X_train_c, y_train_c)\n",
    "\n",
    "\n",
    "print (\"Deploy simulation...\")\n",
    "\n",
    "\n",
    "def get_evaluate_fn(server_model):\n",
    "    \"\"\"Return an evaluation function for server-side evaluation.\"\"\"\n",
    "    # The `evaluate` function will be called after every round\n",
    "    def evaluate(server_round, parameters, config):\n",
    "        # Update model with the latest parameters\n",
    "        server_model.set_weights(parameters)\n",
    "        print (\"Server Evaluating...\")\n",
    "        loss, accuracy = server_model.evaluate(X_test, y_test)\n",
    "        print (\"Server Evaluating complete...\", accuracy, loss)\n",
    "        return loss, {\"accuracy\": accuracy}\n",
    "    return evaluate\n",
    "\n",
    "\n",
    "\n",
    "server_model = Sequential([\n",
    "      #Flatten(input_shape=(79,1)),\n",
    "      Flatten(input_shape=(fl_X_train[0].shape[1] , 1)),\n",
    "      Dense(256, activation='sigmoid'),\n",
    "      Dense(128, activation='sigmoid'), \n",
    "      #Dense(18, activation='sigmoid'),  \n",
    "      Dense(len(label.unique()), activation='sigmoid')\n",
    "    ])\n",
    "server_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create FedAvg strategy\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1.0,\n",
    "        fraction_evaluate=0.5,\n",
    "        min_fit_clients=2, #10,\n",
    "        min_evaluate_clients=2, #5,\n",
    "        min_available_clients=2, #10,\n",
    "        evaluate_fn=get_evaluate_fn(server_model),\n",
    "        #evaluate_metrics_aggregation_fn=weighted_average,\n",
    ")\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_OF_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=NUM_OF_ROUNDS),\n",
    "    strategy=strategy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a082bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fedlearn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a1a946a741f3f6200957dccf23404636c747a9eb7c96c9668153385b834240fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
